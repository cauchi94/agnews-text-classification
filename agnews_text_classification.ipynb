{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f5488cd",
   "metadata": {},
   "source": [
    "# Code Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d1268",
   "metadata": {},
   "source": [
    "## 1. **Library Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707b2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports of libraries to be used\n",
    "\n",
    "from distutils.util import rfc822_escape\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, learning_curve\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Models to be used\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Other models for text classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Reporting\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb3d62f",
   "metadata": {},
   "source": [
    "## 2. **Models Defined & Other Models to Explore**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c93b0d0",
   "metadata": {},
   "source": [
    "All the models that could be used for text classification purposes will be defined with `random_state = 42` to control the shuffling process of the train-test split for all the models defined in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39363d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Models Defined\n",
    "\n",
    "#1. Naive Bayes Model\n",
    "mr_naivebayes = MultinomialNB()\n",
    "\n",
    "#2. Naive Bayes Multiclass\n",
    "mr_naivebayes_multiclass = MultinomialNB()\n",
    "\n",
    "#3. SGD Classifier\n",
    "sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "\n",
    "#4. SGD Classifier Multiclass\n",
    "sgd_multiclass = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "\n",
    "### Other Models to Explore\n",
    "\n",
    "#5. K-Nearest Neigbours  \n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "#6. Support Vector Machine\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "#7. Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "#8. Decision Tree\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "#9. Logistic Regression\n",
    "lr = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d26b10b",
   "metadata": {},
   "source": [
    "## 3. **Pipeline, Parameter Grid & GridSearch Cross-Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7ff3ea",
   "metadata": {},
   "source": [
    "A pipeline combining a text feature extractor with a classifier to be chosen later on is defined.\n",
    "- **CountVectorizer**: <br/>\n",
    "\n",
    "    CountVectorizer converts texts into count frequency. Count Vectors will be helpful in understanding the type of text by the frequency of words in it. <br/> <br/>\n",
    "    But its major disadvantages are: <br/>\n",
    "     - Its inability in identifying more important and less important words for analysis.\n",
    "     - It will just consider words that are abundant in a corpus as the most statistically significant word.\n",
    "     - It also doesn't identify the relationships between words such as linguistic similarity between words. \n",
    "<br/>\n",
    "<br/>    \n",
    "-  **TFIDF**: <br/>\n",
    "    \n",
    "    TFIDF provides a numerical representation of how important a word is for statistical analysis. It is based on the logic that words that are too abundant in a corpus and words that are too rare are both not statistically important for finding a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d240826",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pipeline\n",
    "\n",
    "def define_pipeline(model):\n",
    "    \"\"\"\n",
    "    Function that returns the pipeline for the model that haven been selected.\n",
    "    Params:\n",
    "    - model: object. The model whose pipeline will be returned.\n",
    "    Output:\n",
    "    - pipeline: object. The pipeline object to do the transformations according to the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "                ('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('model', model),\n",
    "                ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f3dec9",
   "metadata": {},
   "source": [
    "- Link for *Stochastic Gradient Descent*: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "- Link for *Naive Bayes*: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "- Link for *K-Nearest Neighbours*: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "- Tips:                          https://medium.datadriveninvestor.com/k-nearest-neighbors-in-python-hyperparameters-tuning-716734bc557f\n",
    "- Link for *Support Vector Machine*: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "- Tips:                            https://medium.com/analytics-vidhya/hyperparameter-tuning-an-svm-a-demonstration-using-hyperparameter-tuning-cross-validation-on-96b05db54e5b#:~:text=What%20is%20hyperparameter%20tuning%20%3F,of%20decrease%20them%20for%20ex.\n",
    "- Link for *Random Forest*: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "- Tips:                   https://developer.spotify.com/documentation/web-api/reference/#/operations/get-track\n",
    "- Link for *Decision Tree*: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "- Tips:                   https://www.kaggle.com/code/gauravduttakiit/hyperparameter-tuning-in-decision-trees/notebook\n",
    "- Link for *Logistic Regression*: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- Tips:                         https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4972d02",
   "metadata": {},
   "source": [
    "Although all of the aforementioned models will have been defined for this project, only NB and SGD will be used to simplify our analysis. In contrast, however, the selection of hyperparameters was based on the most frequently used parameters for each model in data science.\n",
    "\n",
    "Moreover, the definition of two different ngram ranges were made.\n",
    "\n",
    "The lower and upper boundary of the range of n-values for different word n-grams or char n-grams are extracted in a way that all values of n such such that n is bigger than the minimum of n and n is smaller than the maximum of n. Therefore since ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams we decided to explore the different possibilities both incrementally and individually.\n",
    "\n",
    "The analyzer for our models was to be based on words rather than characters, otherwise it would defeat the purpose of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40817c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_param_grid(model: object, ngram: int, method=1):\n",
    "    \"\"\"\n",
    "    Function that returns the parameter grid based on the model that was defined in the pipeline and the n-gram selected.\n",
    "    Params:\n",
    "    - model: object. The model that you want to fine tune with hyperparameters.\n",
    "    - ngram: int. The range of ngrams you want to fine tune the model with.\n",
    "    - method: int. 1 returns the incremented ngram range, 2 returns the individual ngrams.\n",
    "    Output:\n",
    "    - parameters: dict. The parameters defined in the function for the specific model chosen.\n",
    "    IMP: More parameters can be added for each model to add complexity BUT will take much longer!\n",
    "    \"\"\"\n",
    "\n",
    "    ### Choose the ngram method to be taken\n",
    "    \n",
    "    # If method is to be INCREMENTAL ngrams\n",
    "    if method == 1:\n",
    "        # N-gram range for hyperparameter tuning\n",
    "        vect__ngram_range = []\n",
    "        for i in range(ngram):\n",
    "            vect__ngram_range.append((1, i+1))\n",
    "    \n",
    "    # If method is to be INDIVIDUAL ngrams\n",
    "    elif method == 2:\n",
    "        # N-gram range for hyperparameter tuning\n",
    "        vect__ngram_range = []\n",
    "        for i in range(ngram):\n",
    "            vect__ngram_range.append((i+1, i+1))  \n",
    "    \n",
    "    ### Get the model chosen from the pipe\n",
    "    model_chosen = define_pipeline(model)['model']  \n",
    "\n",
    "    ### Obtain parameter grid according to the model chosen\n",
    "    \n",
    "    # Naive Bayes and its Multiclass\n",
    "    if model_chosen == mr_naivebayes or mr_naivebayes_multiclass:\n",
    "        parameters = {\n",
    "    'vect__ngram_range': vect__ngram_range,\n",
    "    'model__alpha': [10 ** -x for x in range(1, 10)],\n",
    "    }\n",
    "\n",
    "    # Stochastic Gradient Descent and its Multiclass    \n",
    "    elif model_chosen == sgd or model_chosen == sgd_multiclass:\n",
    "        parameters = {\n",
    "    'vect__ngram_range': vect__ngram_range,\n",
    "    'model__alpha': [10 ** -x for x in range(1, 10)],\n",
    "    'model__loss': ['hinge', 'log_loss', 'log', 'modified_huber', \n",
    "                    'squared_hinge', 'perceptron', 'squared_error', \n",
    "                    'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "    'model__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'model__fit_intercept': [True, False],\n",
    "    }\n",
    "\n",
    "    # K-Nearest Neighbours    \n",
    "    elif model_chosen == knn:\n",
    "        parameters = {\n",
    "    'vect__ngram_range': vect__ngram_range,\n",
    "    'model__leaf_size': [range(1, 50)],\n",
    "    'model__n_neighbors': [range(1, 30)],\n",
    "    'model__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'model__weights': ['uniform', 'distance'],\n",
    "    'model__p': [1, 2],\n",
    "    }\n",
    "\n",
    "    # Support Vector Machine\n",
    "    elif model_chosen == svm:\n",
    "        parameters = {\n",
    "    'vect__ngram_range': vect__ngram_range,\n",
    "    'model__gamma': [0.1, 1.0, 10, 100, 1000],\n",
    "    'model__C': [0.1, 1.0, 10, 100, 1000],\n",
    "    'model__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'], \n",
    "    }\n",
    "\n",
    "    # Random Forest\n",
    "    elif model_chosen == rf:\n",
    "        parameters = {\n",
    "    'vect__ngram_range': vect__ngram_range,        \n",
    "    'model__n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
    "    'model__max_features': ['auto', 'sqrt'],\n",
    "    'model__max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__bootstrap': [True, False],\n",
    "    }\n",
    "        \n",
    "    # Decision Tree    \n",
    "    elif model_chosen == dt:\n",
    "        parameters = {\n",
    "    'vect__ngram_range': vect__ngram_range,        \n",
    "    'model__max_depth': [2, 3, 5, 10, 20],\n",
    "    'model__min_samples_leaf': [5, 10, 20, 50, 100],\n",
    "    'model__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "    'model__splitter': ['best', 'random'],\n",
    "    'model__max_features': ['auto', 'sqrt', 'log2']\n",
    "    }\n",
    "\n",
    "    # Logistic Regression\n",
    "    elif model_chosen == lr:\n",
    "        parameters = {\n",
    "    'vect__ngram_range': vect__ngram_range, \n",
    "    'model__solvers': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'model__penalty':  ['none', 'l1', 'l2', 'elasticnet'],\n",
    "    'model__C': [0.01, 0.1, 1.0, 10, 100],        \n",
    "    }\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c2048c",
   "metadata": {},
   "source": [
    "Then a function to find the best parameters for both the feature extraction and the classifier was to be defined using a 5-fold cross validation.\n",
    "\n",
    "By utilising cross validation and grid search, we were able to achieve a more meaningful result than with our initial train/test split and minimal tuning. Cross validation is a crucial technique for creating better-fitting models by training and testing on all training dataset components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd3129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gridsearch(model: object, param_grid: dict, scorer = \"accuracy\"):\n",
    "    \"\"\"\n",
    "    Function that returns the parameter grid based on the model that was defined in the pipeline and the n-gram selected.\n",
    "    Params:\n",
    "    - model: object. The model to be fine tuned with hyperparameter selections.\n",
    "    - param_grid: dict. The combination of parameters to be run across the model chosen.\n",
    "    - scorer: str. The metric on which the model is to be evaluated. Default = accuracy.\n",
    "    Output:\n",
    "    - grid: object. The cross-validated scores based on the scorer selected to get the best estimator and parameters.\n",
    "    \"\"\"\n",
    "    pipe = define_pipeline(model)\n",
    "    grid = GridSearchCV(pipe, param_grid, cv = 5, scoring = scorer, return_train_score = True)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0bbe17",
   "metadata": {},
   "source": [
    "## 4. **Convenience Functions, Learning Curves and Report Generators**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0093bb29",
   "metadata": {},
   "source": [
    "Here, we define convenience functions to generate a dataframe that displays the results of the cross-validation experiment and sorts the highest or best estimators at the top. In addition, the feature weights will be sorted as an additional metric in order to comprehend the combination of ngrams and words that influence the classification of the word to the topic.\n",
    "\n",
    "Then, we will use a function that calls the other functions in order to obtain the optimal estimator's results based on the CV and weights.\n",
    "\n",
    "Using a learning curve to explain the accuracy of the fitted best estimator model, the progression of the model as the number of samples increases will be understood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a04d8b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidation_report_df(grid_cv):\n",
    "    \"\"\"\n",
    "    Convenience function that creates a simple dataframe that reports the results of a cross-validation experiment. \n",
    "    Params:\n",
    "    - grid: object. Input cross-validated grid that must be fit.\n",
    "    Output:\n",
    "    - dataframe. The scores sorted by rank of experiment.\n",
    "    \"\"\"\n",
    "    # Pick columns that define each experiment (start with param) and the columns that report mean_test and rank_test results\n",
    "    cols = [c for c in grid_cv.cv_results_ if (c.startswith('param') or c in ['mean_test_score', 'rank_test_score'])]\n",
    "\n",
    "    # Sort original df by rank, and select columns\n",
    "    return pd.DataFrame(grid_cv.cv_results_).sort_values(by='rank_test_score')[cols]\n",
    "\n",
    "def sort_feature_weights(grid, fkey='vect', wkey='model'):\n",
    "    \"\"\" \n",
    "    Convenience function that gets the weights of each words/ngram and orders them from lowest to highest. \n",
    "    Highest being words most associated with the given topic.\n",
    "    Output:\n",
    "    - list. Returns a list of tuples with word/ngram and its weight.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtain the features of the best estimator post fit and the corresponding weights assigned to them\n",
    "    F = grid.best_estimator_[fkey].get_feature_names_out()\n",
    "    try:\n",
    "        W = grid.best_estimator_[wkey].coef_[0]\n",
    "    except AttributeError: #for the mr_naivebayes\n",
    "        W = grid.best_estimator_[wkey].feature_log_prob_[1] \n",
    "    \n",
    "    # Sort the values based on weights\n",
    "    return sorted(zip(F, W), key=lambda fw: fw[1]) \n",
    "\n",
    "def sort_feature_multiclassweights(grid, fkey='vect', wkey='model'):\n",
    "    \"\"\" \n",
    "    Convenience function that gets the weights of each words/ngram and orders them from lowest to highest. \n",
    "    Highest being words most associated with the given topic.\n",
    "    Output:\n",
    "    - list. Returns a list of tuples with word/ngram and its weight.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obrain the featurs of the best estimator post fit and the corresponding weights per topic\n",
    "    F = grid.best_estimator_[fkey].get_feature_names_out()\n",
    "    try:\n",
    "        science = grid.best_estimator_[wkey].coef_[0]\n",
    "        sports = grid.best_estimator_[wkey].coef_[1]\n",
    "        world = grid.best_estimator_[wkey].coef_[2]\n",
    "        business = grid.best_estimator_[wkey].coef_[3]\n",
    "    except AttributeError: #for the mr_naivebayes\n",
    "        science = grid.best_estimator_[wkey].feature_log_prob_[0] \n",
    "        sports = grid.best_estimator_[wkey].feature_log_prob_[1]\n",
    "        world = grid.best_estimator_[wkey].feature_log_prob_[2]\n",
    "        business = grid.best_estimator_[wkey].feature_log_prob_[3]\n",
    "    \n",
    "    # Sort the values based on weights\n",
    "    return sorted(zip(F, science, sports, world, business), key=lambda fw: fw[1]) \n",
    "\n",
    "\n",
    "def apply_modelling(grid, train_data, test_data):\n",
    "    \"\"\"\n",
    "    Convenience function that creates the model for all news types and returns all the info we need to analyze the model\n",
    "    Output:\n",
    "    - cv_report: df report of the CV with the params tried and their scores in the CV.\n",
    "    - score_report: report of the best_estimator's performance on the test data.\n",
    "    - bestestimator_parameters: parameters used in the best estimator.\n",
    "    - bestestimator_weights: word weigths for the best estimator.\n",
    "    \"\"\" \n",
    "    \n",
    "    # Get the cross-validation report, the accuracy score report, the best estimator's parameters & weights and the grid\n",
    "    \n",
    "    targets = [\"science_int\", \"sports_int\", \"world_int\", \"business_int\"]\n",
    "    report_dict = {\n",
    "        \"targets\": targets, \n",
    "        \"cv_report\":[], \n",
    "        \"score_report\":[],\n",
    "        \"bestestimator_parameters\":[],\n",
    "        \"grid\":[], \n",
    "        \"bestestimator_weights\":[]\n",
    "    }\n",
    "\n",
    "    for target in targets:\n",
    "        grid.fit(train_data.text, train_data.loc[:,target])\n",
    "        cv_report = crossvalidation_report_df(grid)\n",
    "        score_report = classification_report(test_data.loc[:,target], \n",
    "            grid.best_estimator_.predict(test_data.text),target_names = [\"others\", target[:-4]])\n",
    "        bestestimator_parameters = grid.best_params_\n",
    "        bestestimator_weights = sort_feature_weights(grid) \n",
    "        report_dict[\"cv_report\"].append(cv_report)\n",
    "        report_dict[\"score_report\"].append(score_report)\n",
    "        report_dict[\"bestestimator_parameters\"].append(bestestimator_parameters)\n",
    "        report_dict[\"grid\"].append(grid)\n",
    "        report_dict[\"bestestimator_weights\"].append(bestestimator_weights)\n",
    "    return report_dict\n",
    "\n",
    "\n",
    "def apply_multiclassmodelling(grid, train_data, test_data):\n",
    "    \"\"\"\n",
    "    Convenience function that creates the model for all news types and returns all the info we need to analyze the model\n",
    "    Output:\n",
    "    - cv_report: df report of the CV with the params tried and their scores in the CV.\n",
    "    - score_report: report of the best_estimator's performance on the test data.\n",
    "    - bestestimator_parameters: parameters used in the best estimator.\n",
    "    - bestestimator_weights: word weigths for the best estimator.\n",
    "    \"\"\" \n",
    "    \n",
    "    targets = [\"label_int\"]\n",
    "    report_dict = {\n",
    "        \"targets\": targets, \n",
    "        \"cv_report\":[], \n",
    "        \"score_report\":[],\n",
    "        \"bestestimator_parameters\":[],\n",
    "        \"grid\":[], \n",
    "        \"bestestimator_weights\":[]\n",
    "    }\n",
    "\n",
    "    for target in targets:\n",
    "        grid.fit(train_data.text, train_data.loc[:,target])\n",
    "        cv_report = crossvalidation_report_df(grid)\n",
    "        score_report = classification_report(\n",
    "            test_data.loc[:,target], \n",
    "            grid.best_estimator_.predict(test_data.text),\n",
    "            target_names = [\"Science\", \"Sports\", \"World\", \"Business\"])\n",
    "        bestestimator_parameters = grid.best_params_\n",
    "        bestestimator_weights = sort_feature_multiclassweights(grid) #order of weights is like in tabl (science first,...)\n",
    "        report_dict[\"cv_report\"].append(cv_report)\n",
    "        report_dict[\"score_report\"].append(score_report)\n",
    "        report_dict[\"bestestimator_parameters\"].append(bestestimator_parameters)\n",
    "        report_dict[\"grid\"].append(grid)\n",
    "        report_dict[\"bestestimator_weights\"].append(bestestimator_weights)\n",
    "    return report_dict\n",
    "\n",
    "\n",
    "def create_learningcurve(train_data, test_data, selected_estimator, loss, report_dict):\n",
    "    \"\"\"\n",
    "    Function that creates the learning curve for the selected estimator.\n",
    "    Output:\n",
    "    - dict. The report_dict with an added column for the learning curve.\n",
    "    \"\"\"\n",
    "    \n",
    "    targets = [\"science_int\", \"sports_int\", \"world_int\", \"business_int\"]\n",
    "    report_dict[\"training_curve\"] = []\n",
    "    for target in targets:\n",
    "        \n",
    "        #Takes a loss function and a model and find the performance for a given amount of data for a specific data set\n",
    "        training_curve = np.array([[0,0]])\n",
    "        n_samples = 10\n",
    "        while n_samples < train_data.shape[0]:\n",
    "            print (n_samples)\n",
    "            i=0\n",
    "            new_model = selected_estimator.fit(\n",
    "                train_data.loc[:n_samples,\"text\"], train_data.loc[:n_samples,target]\n",
    "                )\n",
    "            y_pred = new_model.predict(test_data.text)\n",
    "            score = loss(test_data.loc[:,target], y_pred)\n",
    "            training_curve = np.append(training_curve,[[n_samples, score]], axis = 0)\n",
    "            n_samples *= 2\n",
    "        report_dict[\"training_curve\"].append(training_curve)\n",
    "    return report_dict\n",
    "\n",
    "def create_multiclasslearningcurve(train_data, test_data, selected_estimator, loss, report_dict):\n",
    "    \"\"\"\n",
    "    Function that creates the learning curve for the selected estimator.\n",
    "    Output:\n",
    "    - dict. The report_dict with an added column for the learning curve.\n",
    "    \"\"\"\n",
    "    \n",
    "    targets = [\"label_int\"]\n",
    "    report_dict[\"training_curve\"] = []\n",
    "    for target in targets:\n",
    "        \n",
    "        #Takes a loss function and a model and find the performance for a given amount of data for a specific data set\n",
    "        training_curve = np.array([[0,0]])\n",
    "        n_samples = 10\n",
    "        while n_samples < train_data.shape[0]:\n",
    "            print (n_samples)\n",
    "            i=0\n",
    "            new_model = selected_estimator.fit(\n",
    "                train_data.loc[:n_samples,\"text\"], train_data.loc[:n_samples,target]\n",
    "                )\n",
    "            y_pred = new_model.predict(test_data.text)\n",
    "            score = loss(test_data.loc[:,target], y_pred)\n",
    "            training_curve = np.append(training_curve,[[n_samples, score]], axis = 0)\n",
    "            n_samples *= 2\n",
    "        report_dict[\"training_curve\"].append(training_curve)\n",
    "    return report_dict\n",
    "\n",
    "\n",
    "##Analysis functions that can be used to dig into the models##\n",
    "\n",
    "def show_cv_report(report_dict):\n",
    "    \"\"\"\n",
    "    Convenience function to print results to analyze them.\n",
    "    Output:\n",
    "    - dict: The report dictionary we would like to analyze.\n",
    "    \"\"\"\n",
    "    \n",
    "    targets = [\"science_int\", \"sports_int\", \"world_int\", \"business_int\"]\n",
    "    for index, value in enumerate(targets):\n",
    "        print(\"\\n CV report for {}\\n\".format(value[:-4]))\n",
    "        print(report_dict[\"cv_report\"][index])\n",
    "\n",
    "def show_classification_report(report_dict):\n",
    "    ''' Convenience function to print results to analyze them\n",
    "    report_dict: report dictionary we would like to analyze'''\n",
    "    targets = [\"science_int\", \"sports_int\", \"world_int\", \"business_int\"]\n",
    "    for index, value in enumerate(targets):\n",
    "        print(\"\\n Classification report of best estimator for {}\\n\".format(value[:-4]))\n",
    "        print(report_dict[\"score_report\"][index])\n",
    "\n",
    "def show_best_estimator(report_dict):\n",
    "    ''' Convenience function to print results to analyze them\n",
    "    report_dict: report dictionary we would like to analyze'''\n",
    "    targets = [\"science_int\", \"sports_int\", \"world_int\", \"business_int\"]\n",
    "    for index, value in enumerate(targets):\n",
    "        print(\"\\n Parameters of best estimator for {}\\n\".format(value[:-4]))\n",
    "        print(report_dict[\"bestestimator_parameters\"][index])    \n",
    "\n",
    "def show_word_weights(report_dict):\n",
    "    ''' Convenience function to print results to analyze them\n",
    "    report_dict: report dictionary we would like to analyze'''\n",
    "    targets = [\"science_int\", \"sports_int\", \"world_int\", \"business_int\"]\n",
    "    for index, value in enumerate(targets):\n",
    "        print(\"\\n Top words for {} \\n\".format(value[:-4]))\n",
    "        print(report_dict[\"bestestimator_weights\"][index][-20:])\n",
    "        print(\"\\n Worst words for {}\\n\".format(value[:-4]))\n",
    "        print(report_dict[\"bestestimator_weights\"][index][:20])      \n",
    "\n",
    "def show_multiclassword_weights(multiclass_report_dict):\n",
    "    ''' Convenience function to print results to analyze them\n",
    "    multiclass_report_dict: report dictionary we would like to analyze'''\n",
    "    targets = [\"science_int\", \"sports_int\", \"world_int\", \"business_int\"]\n",
    "    for index, value in enumerate(targets):\n",
    "        sorted_words = sorted(multiclass_report_dict[\"bestestimator_weights\"][0], key=lambda fw: fw[index + 1])\n",
    "        print(\"\\n Top words for {}\\n\".format(value[:-4]))\n",
    "        print(sorted_words[-20:])\n",
    "        print(\"\\n Worst words for {}\\n\".format(value[:-4]))\n",
    "        print(sorted_words[:20])      \n",
    "\n",
    "\n",
    "def show_learning_curve(report_dict):\n",
    "    ''' Convenience function to print results to analyze them\n",
    "    report_dict: report dictionary we would like to analyze'''\n",
    "    targets = [\"science_int\", \"sports_int\", \"world_int\", \"business_int\"]\n",
    "    for index, value in enumerate(targets):\n",
    "        print(\"\\n Learning curve for {}\\n\".format(value[:-4]))\n",
    "        x = report_dict[\"training_curve\"][index][:,0]\n",
    "        y = report_dict[\"training_curve\"][index][:,1]\n",
    "        plt.plot(x,y, label = value[:-4])\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"# of samples in training\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def show_multiclasslearning_curve(multiclass_report_dict):\n",
    "    ''' Convenience function to print results to analyze them\n",
    "    multiclass_report_dict: report dictionary we would like to analyze'''\n",
    "    targets = [\"label_int\"]\n",
    "    for index, value in enumerate(targets):\n",
    "        print(\"\\n Learning curve for {}\\n\".format(value))\n",
    "        x = multiclass_report_dict[\"training_curve\"][index][:,0]\n",
    "        y = multiclass_report_dict[\"training_curve\"][index][:,1]\n",
    "        plt.plot(x,y, label = \"Overall\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"# of samples in training\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_precision_recall_curve(report_dicts, test_data):\n",
    "    '''Convenience function that prints all precision recall curves of the models to be analyzed\n",
    "    report_dicts: list of report dictionaries for which we would like to analyze the best models\n",
    "    test_data: df with the test data provided\n",
    "    returns a plot with the charts'''\n",
    "\n",
    "    #create a list of the models to analyze\n",
    "    models = []\n",
    "    for report_dict in report_dicts:\n",
    "        models.append(report_dict[\"grid\"][0])\n",
    "\n",
    "    targets = [\"science_int\", \"sports_int\", \"world_int\", \"business_int\"]\n",
    "\n",
    "    #Create a chart for each prediction target\n",
    "    for target_index, target in enumerate(targets):\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.title('Precision-Recall curve for {}'.format(target))\n",
    "        plt.xlabel('precision')\n",
    "        plt.ylabel('recall')\n",
    "\n",
    "\n",
    "        colors = [\"green\", \"red\", \"orange\", \"black\", \"blue\", \"pink\"]\n",
    "        markers = ['o', 'v', '^', '<', '>', '8']\n",
    "        #Create the scatter for each model\n",
    "        for model_index, model in enumerate(models):\n",
    "            pr = precision_recall_curve(\n",
    "                test_data.loc[:,target],\n",
    "                model.decision_function(test_data.text),\n",
    "                pos_label=1)\n",
    "            plt.scatter(y=pr[0], x=pr[1], label='Model {}'.format(model_index), alpha = 0.5, linewidths = 0.5, color = colors[model_index], marker = markers[model_index])\n",
    "          \n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def show_multiclassprecision_recall_curve(multiclass_report_dicts, test_data):\n",
    "    '''Convenience function that prints all precision recall curves of the models to be analyzed\n",
    "    multiclass_report_dicts: list of report dictionaries for which we would like to analyze the best models\n",
    "    test_data: df with the test data provided\n",
    "    returns a plot with the charts'''\n",
    "\n",
    "    #create a list of the models to analyze\n",
    "    models = []\n",
    "    for report_dict in multiclass_report_dicts:\n",
    "        models.append(report_dict[\"grid\"][0])\n",
    "\n",
    "    targets = [\"science_int\", \"sports_int\", \"world_int\", \"business_int\"]\n",
    "    fig, axes = plt.subplots(2,4)\n",
    "\n",
    "    #Create a chart for each prediction target\n",
    "    for target_index, target in enumerate(targets):\n",
    "\n",
    "        axes[1, target_index].set_title('Precision-Recall curve for {}'.format(target))\n",
    "        axes[1, target_index].set_xlabel('precision')\n",
    "        axes[1, target_index].set_ylabel('recall')\n",
    "        axes[1, target_index].grid(True)\n",
    "\n",
    "\n",
    "        colors = [\"green\", \"red\", \"orange\", \"black\", \"blue\", \"pink\"]\n",
    "        markers = ['o', 'v', '^', '<', '>', '8']\n",
    "        #Create the scatter for each model\n",
    "        for model_index, model in enumerate(models):\n",
    "            pr = precision_recall_curve(\n",
    "                test_data.loc[:,target],\n",
    "                model.decision_function(test_data.text)[:,target_index],\n",
    "                pos_label=1)\n",
    "            axes[1, target_index].scatter(y=pr[0], x=pr[1], label='Model {}'.format(model_index), alpha = 0.5, linewidths = 0.5, color = colors[model_index], marker = markers[model_index])\n",
    "        \n",
    "    fig.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7f2006",
   "metadata": {},
   "source": [
    "## 5. **Base Models (only Unigram)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1437c071",
   "metadata": {},
   "source": [
    "### a. *Models for each Target*: **Stochastic Gradient Descent & Multinomial Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab31352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Load data\n",
    "    train = pd.read_csv(\"./06_Session 6 - NLP/Advanced_AI_NLP/agnews_train.csv\")\n",
    "    test = pd.read_csv(\"./06_Session 6 - NLP/Advanced_AI_NLP/agnews_test.csv\")\n",
    "\n",
    "    ####### Models for each target ######\n",
    "\n",
    "    #Define model (Base)\n",
    "    param_grid = {\n",
    "        'vect__ngram_range': [(1,1)],\n",
    "        'model__alpha': [(1e-9)]\n",
    "        }    \n",
    "\n",
    "    #create grids for basic sgd and nb\n",
    "    grid_sgd = define_gridsearch(model = sgd, param_grid = param_grid, scorer = \"accuracy\")\n",
    "    grid_nb = define_gridsearch(model = sgd, param_grid = param_grid, scorer = \"accuracy\")\n",
    "    \n",
    "    #Create reporting data for sgd and nb\n",
    "    report_dict_sgd = apply_modelling(grid_sgd, train, test)\n",
    "    report_dict_nb = apply_modelling(grid_nb, train, test)\n",
    "    report_dict_sgd = create_learningcurve(train, test, grid_sgd.best_estimator_, accuracy_score, report_dict_sgd) #can choose any estimator; don't have to choose best_estimator here\n",
    "    report_dict_nb = create_learningcurve(train, test, grid_nb.best_estimator_, accuracy_score, report_dict_nb) #can choose any estimator; don't have to choose best_estimator here\n",
    "\n",
    "\n",
    "    #Saving the results of the testing into \n",
    "    df = pd.DataFrame(report_dict_sgd) \n",
    "    df.to_csv (r\"./06_Session 6 - NLP/Advanced_AI_NLP/base_test_series_data.csv\",index = False, header=True)\n",
    "\n",
    "    ##Analysis##\n",
    "\n",
    "    #looking at the performance of the different hyperparameters in the gridsearch\n",
    "    print(\"CV report for each target\")\n",
    "    show_cv_report(report_dict_sgd)\n",
    "\n",
    "    #Looking at the classification report of the best estimator\n",
    "    print(\"Classification reports of best estimators\")\n",
    "    show_classification_report(report_dict_sgd)\n",
    "\n",
    "    #Looking at the parameters of the best estimator\n",
    "    print(\"Parameters of best estimators\")\n",
    "    show_best_estimator(report_dict_sgd)\n",
    "\n",
    "    #Look at word weights\n",
    "    print(\"Word weights of best estimators for each target\")\n",
    "    show_word_weights(report_dict_sgd)\n",
    "\n",
    "    #looking at the learning curve\n",
    "    print(\"Leanring curve of models\")\n",
    "    show_learning_curve(report_dict_sgd)\n",
    "\n",
    "    #Comparing the precision recall curve of different models\n",
    "    report_dicts = [report_dict_sgd, report_dict_nb]\n",
    "    show_precision_recall_curve(report_dicts, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdccdb79",
   "metadata": {},
   "source": [
    "### b. *Multiclass*: **Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab92c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Load data\n",
    "    train = pd.read_csv(\"./06_Session 6 - NLP/Advanced_AI_NLP/agnews_train.csv\")\n",
    "    test = pd.read_csv(\"./06_Session 6 - NLP/Advanced_AI_NLP/agnews_test.csv\") \n",
    "        \n",
    "    ####### Multiclass models ######\n",
    "\n",
    "    #Define model\n",
    "    param_grid = {\n",
    "        'vect__ngram_range': [(1,1)],\n",
    "        'model__alpha': [(1e-9)]\n",
    "        }    \n",
    "    multiclass_grid = define_gridsearch(model = sgd_multiclass, param_grid = param_grid, scorer = \"accuracy\") #Not sure whether multiclass would work with the bayes\n",
    "\n",
    "    #Create reporting data\n",
    "    multiclass_report_dict = apply_multiclassmodelling(multiclass_grid, train, test)\n",
    "    multiclass_report_dict = create_multiclasslearningcurve(train, test, multiclass_grid.best_estimator_, accuracy_score, multiclass_report_dict) #can choose any estimator; don't have to choose best_estimator here\n",
    "\n",
    "    #Saving the results of the testing into \n",
    "    multiclass_df = pd.DataFrame(multiclass_report_dict) \n",
    "    multiclass_df.to_csv (\n",
    "        r\"./06_Session 6 - NLP/Advanced_AI_NLP/multiclass_base_test_series_data.csv\",\n",
    "         index = False, header=True\n",
    "         )\n",
    "\n",
    "    ##Analysis##\n",
    "\n",
    "    #looking at the performance of the different hyperparameters in the gridsearch\n",
    "    print(\"\\n Gridsearch CV report \\n\")\n",
    "    print(multiclass_report_dict[\"cv_report\"][0])      \n",
    "\n",
    "    #Looking at the classification report of the best estimator\n",
    "    print(\"\\n Classification report of best estimator \\n\")\n",
    "    print(multiclass_report_dict[\"score_report\"][0])\n",
    "\n",
    "    #Looking at the parameters of the best estimator\n",
    "    print(\"\\n Parameters of best estimator \\n\")\n",
    "    print(multiclass_report_dict[\"bestestimator_parameters\"][0])\n",
    "\n",
    "    #Look at word weights\n",
    "    print(\"\\n Word weights for each target \\n\")\n",
    "    show_multiclassword_weights(multiclass_report_dict)\n",
    "\n",
    "    #looking at the learning curve\n",
    "    print(\"\\n Learning curve of models \\n\")\n",
    "    show_multiclasslearning_curve(multiclass_report_dict)\n",
    "\n",
    "    #Comparing the precision recall curve of different models\n",
    "    multiclass_report_dicts = [multiclass_report_dict]\n",
    "    show_multiclassprecision_recall_curve(multiclass_report_dicts, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad684f",
   "metadata": {},
   "source": [
    "## 6. *Models for each target*: **Hyperparameter Tuned Models (up to Fivegram)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51df6813",
   "metadata": {},
   "source": [
    "In this section, we will define models that will output the best parameters and ngrams for each individual target: science, sports, world, and business. As each of its models has its own context, this should allow for greater precision, as a specific ngram range may only be applicable to one topic and not all of them. This should allow for greater average precision than the multiclass we will define later.\n",
    "\n",
    "We chose fivegram as our maximum range because, for text classification purposes in data science, fourgram is typically sufficient to achieve the highest degree of precision. Additionally, this depends on the length of each topic's sentences. A higher ngram range could capture almost the entire sentence and therefore does not allow for a more nuanced understanding of the sentence's constituent parts.\n",
    "\n",
    "In the models described below, it was almost always the case that the optimal ngram range was up to a unigram or a bigram, with the rare exception of a trigram. However, the F1 score tends to decrease thereafter, indicating that the selection of fivegram was adequate.\n",
    "\n",
    "The slightly superior performance of the Multinomial Naive Bayes model could be attributed to the fact that the dataset was perfectly balanced; otherwise, the model would have returned different results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f952cb86",
   "metadata": {},
   "source": [
    "### a. **Multinomial Naive Bayes**\n",
    "> i. **Incremental (from unigram only up to range of unigram to fivegram)** <br/>\n",
    "> ii. **Individual (from unigram only up to fivegram only)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2abbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Load data\n",
    "    train = pd.read_csv(\"./06_Session 6 - NLP/Advanced_AI_NLP/agnews_train.csv\")\n",
    "    test = pd.read_csv(\"./06_Session 6 - NLP/Advanced_AI_NLP/agnews_test.csv\") \n",
    "\n",
    "    ####### Hyperparameter tuning and ngram models ######\n",
    "    \n",
    "    ### i\n",
    "\n",
    "    #Define Multinomial Naive Bayes model using incremental method up to fivegram\n",
    "    param_grid = define_param_grid(model = mr_naivebayes, ngram = 5, method = 1)\n",
    "    grid = define_gridsearch(model = mr_naivebayes, param_grid = param_grid, scorer = \"accuracy\")\n",
    "    \n",
    "    #Create reporting data\n",
    "    report_dict = apply_modelling(grid, train, test)\n",
    "    report_dict_nb_1 = create_learningcurve(train, test, grid.best_estimator_, accuracy_score, report_dict) #can choose any estimator; don't have to choose best_estimator here\n",
    "\n",
    "    #Saving the results of the testing into \n",
    "    df = pd.DataFrame(report_dict_nb_1) \n",
    "    df.to_csv (r\"./06_Session 6 - NLP/Advanced_AI_NLP/NB_tuned_test_series_data.csv\",index = False, header=True)\n",
    "\n",
    "    ##Analysis##\n",
    "\n",
    "    #looking at the performance of the different hyperparameters in the gridsearch\n",
    "    print(\"CV report for each target\")\n",
    "    show_cv_report(report_dict_nb_1)\n",
    "\n",
    "    #Looking at the classification report of the best estimator\n",
    "    print(\"Classification reports of best estimators\")\n",
    "    show_classification_report(report_dict_nb_1)\n",
    "\n",
    "    #Looking at the parameters of the best estimator\n",
    "    print(\"Parameters of best estimators\")\n",
    "    show_best_estimator(report_dict_nb_1)\n",
    "\n",
    "    #Look at word weights\n",
    "    print(\"Word weights of best estimators for each target\")\n",
    "    show_word_weights(report_dict_nb_1)\n",
    "\n",
    "    #looking at the learning curve\n",
    "    print(\"Leanring curve of models\")\n",
    "    show_learning_curve(report_dict_nb_1)\n",
    "    \n",
    "    ### ii\n",
    "\n",
    "    #Define Multinomial Naive Bayes model using individual method up to fivegram\n",
    "    param_grid = define_param_grid(model = mr_naivebayes, ngram = 5, method = 2)\n",
    "    grid = define_gridsearch(model = mr_naivebayes, param_grid = param_grid, scorer = \"accuracy\")\n",
    "    \n",
    "    #Create reporting data\n",
    "    report_dict = apply_modelling(grid, train, test)\n",
    "    report_dict_nb_2 = create_learningcurve(train, test, grid.best_estimator_, accuracy_score, report_dict) #can choose any estimator; don't have to choose best_estimator here\n",
    "\n",
    "    #Saving the results of the testing into \n",
    "    df = pd.DataFrame(report_dict_nb_2) \n",
    "    df.to_csv (r\"./06_Session 6 - NLP/Advanced_AI_NLP/NB_tuned_test_series_data_#2.csv\",index = False, header=True)\n",
    "\n",
    "    ##Analysis##\n",
    "\n",
    "    #looking at the performance of the different hyperparameters in the gridsearch\n",
    "    print(\"CV report for each target\")\n",
    "    show_cv_report(report_dict_nb_2)\n",
    "\n",
    "    #Looking at the classification report of the best estimator\n",
    "    print(\"Classification reports of best estimators\")\n",
    "    show_classification_report(report_dict_nb_2)\n",
    "\n",
    "    #Looking at the parameters of the best estimator\n",
    "    print(\"Parameters of best estimators\")\n",
    "    show_best_estimator(report_dict_nb_2)\n",
    "\n",
    "    #Look at word weights\n",
    "    print(\"Word weights of best estimators for each target\")\n",
    "    show_word_weights(report_dict_nb_2)\n",
    "\n",
    "    #looking at the learning curve\n",
    "    print(\"Leanring curve of models\")\n",
    "    show_learning_curve(report_dict_nb_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c902d4",
   "metadata": {},
   "source": [
    "### b. **Stochastic Gradient Descent**\n",
    "> i. **Incremental (from unigram only up to range of unigram to fivegram)** <br/>\n",
    "> ii. **Individual (from unigram only up to fivegram only)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f0eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Load data\n",
    "    train = pd.read_csv(\"./06_Session 6 - NLP/Advanced_AI_NLP/agnews_train.csv\")\n",
    "    test = pd.read_csv(\"./06_Session 6 - NLP/Advanced_AI_NLP/agnews_test.csv\")\n",
    "        \n",
    "    ### i\n",
    "\n",
    "    #Define Stochastic Gradient Descent model using incremental method up to fivegram\n",
    "    param_grid = define_param_grid(model = sgd, ngram = 5, method = 1)\n",
    "    grid = define_gridsearch(model = sgd, param_grid = param_grid, scorer = \"accuracy\")\n",
    "    \n",
    "    #Create reporting data\n",
    "    report_dict = apply_modelling(grid, train, test)\n",
    "    report_dict_sgd_1 = create_learningcurve(train, test, grid.best_estimator_, accuracy_score, report_dict) #can choose any estimator; don't have to choose best_estimator here\n",
    "\n",
    "    #Saving the results of the testing into \n",
    "    df = pd.DataFrame(report_dict_sgd_1) \n",
    "    df.to_csv (r\"./06_Session 6 - NLP/Advanced_AI_NLP/SGD_tuned_test_series_data_#2.csv\",index = False, header=True)\n",
    "\n",
    "    ##Analysis##\n",
    "\n",
    "    #looking at the performance of the different hyperparameters in the gridsearch\n",
    "    print(\"CV report for each target\")\n",
    "    show_cv_report(report_dict_sgd_1)\n",
    "\n",
    "    #Looking at the classification report of the best estimator\n",
    "    print(\"Classification reports of best estimators\")\n",
    "    show_classification_report(report_dict_sgd_1)\n",
    "\n",
    "    #Looking at the parameters of the best estimator\n",
    "    print(\"Parameters of best estimators\")\n",
    "    show_best_estimator(report_dict_sgd_1)\n",
    "\n",
    "    #Look at word weights\n",
    "    print(\"Word weights of best estimators for each target\")\n",
    "    show_word_weights(report_dict_sgd_1)\n",
    "\n",
    "    #looking at the learning curve\n",
    "    print(\"Leanring curve of models\")\n",
    "    show_learning_curve(report_dict_sgd_1)\n",
    "    \n",
    "    ### ii\n",
    "\n",
    "    #Define Stochastic Gradient Descent model using individual method up to fivegram\n",
    "    param_grid = define_param_grid(model = sgd, ngram = 5, method = 2)\n",
    "    grid = define_gridsearch(model = sgd, param_grid = param_grid, scorer = \"accuracy\")\n",
    "    \n",
    "    #Create reporting data\n",
    "    report_dict = apply_modelling(grid, train, test)\n",
    "    report_dict_sgd_2 = create_learningcurve(train, test, grid.best_estimator_, accuracy_score, report_dict) #can choose any estimator; don't have to choose best_estimator here\n",
    "\n",
    "    #Saving the results of the testing into \n",
    "    df = pd.DataFrame(report_dict_sgd_2) \n",
    "    df.to_csv (r\"./06_Session 6 - NLP/Advanced_AI_NLP/SGD_tuned_test_series_data.csv\",index = False, header=True)\n",
    "\n",
    "    ##Analysis##\n",
    "\n",
    "    #looking at the performance of the different hyperparameters in the gridsearch\n",
    "    print(\"CV report for each target\")\n",
    "    show_cv_report(report_dict_sgd_2)\n",
    "\n",
    "    #Looking at the classification report of the best estimator\n",
    "    print(\"Classification reports of best estimators\")\n",
    "    show_classification_report(report_dict_sgd_2)\n",
    "\n",
    "    #Looking at the parameters of the best estimator\n",
    "    print(\"Parameters of best estimators\")\n",
    "    show_best_estimator(report_dict_sgd_2)\n",
    "\n",
    "    #Look at word weights\n",
    "    print(\"Word weights of best estimators for each target\")\n",
    "    show_word_weights(report_dict_sgd_2)\n",
    "\n",
    "    #looking at the learning curve\n",
    "    print(\"Leanring curve of models\")\n",
    "    show_learning_curve(report_dict_sgd_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c01f0cd",
   "metadata": {},
   "source": [
    "### c. Compare Precision-Recall Curve for Models in 6 (ai, aii, bi, bii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824a1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing the precision recall curve of different models\n",
    "report_dicts = [report_dict_nb_1, report_dict_nb_2, report_dict_sgd_1, report_dict_sgd_2]\n",
    "show_precision_recall_curve(report_dicts, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de8355d",
   "metadata": {},
   "source": [
    "## 7. *Multiclass Models*: *Hyperparameter Tuned (up to Fivegram)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e30ca6",
   "metadata": {},
   "source": [
    "In this section, we will define multiclass classification which is a text classification task with more than two classes or targets. Each data sample can be classified into one of the classes. However, a data sample cannot belong to more than one class simultaneously.\n",
    "\n",
    "In this case, the model will classify news headlines into the corresponding categories: science, sports, world and business using only one combination of parameters. \n",
    "\n",
    "As was expected, the accuracy was lower than those of the single class models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20672b0",
   "metadata": {},
   "source": [
    "### a. **Multinomial Naive Bayes**\n",
    "> **Incremental (from unigram only up to range of unigram to fivegram)** <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91a2028",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Load data\n",
    "    train = pd.read_csv(\"./06_Session 6 - NLP/Advanced_AI_NLP/agnews_train.csv\")\n",
    "    test = pd.read_csv(\"./06_Session 6 - NLP/Advanced_AI_NLP/agnews_test.csv\") \n",
    "\n",
    "    ####### Hyperparameter tuning and ngram models ######\n",
    "\n",
    "    #Define Multinomial Naive Bayes model using incremental method up to fivegram\n",
    "    param_grid = define_param_grid(model = mr_naivebayes_multiclass, ngram = 5, method = 1)\n",
    "    grid = define_gridsearch(model = mr_naivebayes_multiclass, param_grid = param_grid, scorer = \"accuracy\")\n",
    "    \n",
    "    #Create reporting data\n",
    "    report_dict = apply_modelling(grid, train, test)\n",
    "    report_dict_multi_nb = create_learningcurve(train, test, grid.best_estimator_, accuracy_score, report_dict) #can choose any estimator; don't have to choose best_estimator here\n",
    "\n",
    "    #Saving the results of the testing into \n",
    "    df = pd.DataFrame(report_dict_multi_nb) \n",
    "    df.to_csv (r\"./06_Session 6 - NLP/Advanced_AI_NLP/NB_Multiiclass_tuned_test_series_data.csv\",index = False, header=True)\n",
    "\n",
    "    ##Analysis##\n",
    "\n",
    "    #looking at the performance of the different hyperparameters in the gridsearch\n",
    "    print(\"CV report for each target\")\n",
    "    show_cv_report(report_dict_multi_nb)\n",
    "\n",
    "    #Looking at the classification report of the best estimator\n",
    "    print(\"Classification reports of best estimators\")\n",
    "    show_classification_report(report_dict_multi_nb)\n",
    "\n",
    "    #Looking at the parameters of the best estimator\n",
    "    print(\"Parameters of best estimators\")\n",
    "    show_best_estimator(report_dict_multi_nb)\n",
    "\n",
    "    #Look at word weights\n",
    "    print(\"Word weights of best estimators for each target\")\n",
    "    show_word_weights(report_dict_multi_nb)\n",
    "\n",
    "    #looking at the learning curve\n",
    "    print(\"Leanring curve of models\")\n",
    "    show_learning_curve(report_dict_multi_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8555bb97",
   "metadata": {},
   "source": [
    "### b. **Stochastic Gradient Descent**\n",
    "> **Incremental (from unigram only up to range of unigram to fivegram)** <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f241dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Load data\n",
    "    train = pd.read_csv(\"./06_Session 6 - NLP/Advanced_AI_NLP/agnews_train.csv\")\n",
    "    test = pd.read_csv(\"./06_Session 6 - NLP/Advanced_AI_NLP/agnews_test.csv\")\n",
    "\n",
    "    #Define Stochastic Gradient Descent model using incremental method up to fivegram\n",
    "    param_grid = define_param_grid(model = sgd_multiclass, ngram = 5, method = 1)\n",
    "    grid = define_gridsearch(model = sgd_multiclass, param_grid = param_grid, scorer = \"accuracy\")\n",
    "    \n",
    "    #Create reporting data\n",
    "    report_dict = apply_modelling(grid, train, test)\n",
    "    report_dict_multi_sgd = create_learningcurve(train, test, grid.best_estimator_, accuracy_score, report_dict) #can choose any estimator; don't have to choose best_estimator here\n",
    "\n",
    "    #Saving the results of the testing into \n",
    "    df = pd.DataFrame(report_dict_multi_sgd) \n",
    "    df.to_csv (r\"./06_Session 6 - NLP/Advanced_AI_NLP/SGD_Multiclass_tuned_test_series_data.csv\",index = False, header=True)\n",
    "\n",
    "    ##Analysis##\n",
    "\n",
    "    #looking at the performance of the different hyperparameters in the gridsearch\n",
    "    print(\"CV report for each target\")\n",
    "    show_cv_report(report_dict_multi_sgd)\n",
    "\n",
    "    #Looking at the classification report of the best estimator\n",
    "    print(\"Classification reports of best estimators\")\n",
    "    show_classification_report(report_dict_multi_sgd)\n",
    "\n",
    "    #Looking at the parameters of the best estimator\n",
    "    print(\"Parameters of best estimators\")\n",
    "    show_best_estimator(report_dict_multi_sgd)\n",
    "\n",
    "    #Look at word weights\n",
    "    print(\"Word weights of best estimators for each target\")\n",
    "    show_word_weights(report_dict_multi_sgd)\n",
    "\n",
    "    #looking at the learning curve\n",
    "    print(\"Leanring curve of models\")\n",
    "    show_learning_curve(report_dict_multi_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a00b94f",
   "metadata": {},
   "source": [
    "### c. Compare Precision-Recall Curve for Models in 7 (a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70274250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing the precision recall curve of different multiclass models\n",
    "multiclass_report_dicts = [report_dict_multi_nb, report_dict_multi_sgd]\n",
    "show_multiclassprecision_recall_curve(multiclass_report_dicts, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
